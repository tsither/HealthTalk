
'''This code sets up a retrieval-augmented generation system using Anyscale's LLM and Pinecone for document retrieval, 
making it suitable for querying and generating responses based on the uploaded dataset.
'''

### STEP 1 : Set up your environment 

import os
import time
import torch
import tensorflow
import flax
import pandas as pd 

#loaders
from langchain_community.document_loaders import PyMuPDFLoader
#text splitters 
from langchain.text_splitter import CharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
#anyscale embeddings
from langchain_community.embeddings import AnyscaleEmbeddings
#vector storage 
from pinecone import Pinecone, ServerlessSpec
from langchain_community.vectorstores import Pinecone as PineconeStore 
#chat anyscale
from langchain_community.chat_models import ChatAnyscale
#chat templates 
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import PromptTemplate
#retrieval chains 
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Set up API keys | Anyscale & Pinecone  
os.environ["ANYSCALE_API_KEY"] = ""
os.environ["PINECONE_API_KEY"] = ""
os.environ["TOKENIZERS_PARALLELISM"] = "false"

### STEP 2 : Store knowledge in Pinecone 

## 2.1 Load your data [via Langchain loaders OR Content Extractor]

# 2.1.1.1 Langchain Loaders | PDF 
pdf_loader = PyMuPDFLoader('Sample_Blood.pdf')
Blood_reports = pdf_loader.load()

# 2.1.1.2 Langchain Loaders | CSV 

# 2.1.2 OCR

## 2.2 Text splitting [Based on chunks] -> later can split into more sophisticated methods 
# experiment/read up on different methods! 

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
docs = text_splitter.split_documents(Blood_reports)

## 2.3 Initalize a LangChain embedding object [Anyscale]
api_key = os.getenv("ANYSCALE_API_KEY")
embeddings = AnyscaleEmbeddings(
    anyscale_api_key=api_key, model="thenlper/gte-large"
)

## 2.4 Create a serverless index in Pinecone for embedding storage 
# [Dims: 1024 & Metric: Cosine to match Anyscale]

pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

index_name = "test"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name="test",
        dimension=1024, 
        metric="cosine", 
        spec=ServerlessSpec(
            cloud="aws", 
            region="us-east-1"
        ) 
    ) 

## 2.5 Embed each chunk and upsert the embeddings into a distinct namespace
# Namespaces let you partition records within an index

namespace = "TEST"
pineconeVector = PineconeStore.from_documents(
    documents=docs,
    index_name=index_name,
    embedding=embeddings, 
    namespace=namespace 
)

time.sleep(1)


### STEP 3: Retrieve & Generate 

# Chain: question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.

## 3.1 Initialize the language model with Anyscale -> need to switch model to anyscale chat compatible 

llm = ChatAnyscale(anyscale_api_key=os.getenv('ANYSCALE_API_KEY'), model = 'google/gemma-7b-it')

# Initialize the Pinecone retriever
retriever = pineconeVector.as_retriever()

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}

Helpful Answer:"""
custom_rag_prompt = PromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_rag_prompt
    | llm
    | StrOutputParser()
)
Q1 ="What is the hemoglobin count of Yash?"
query_with_knowledge = rag_chain.invoke(Q1)
print(query_with_knowledge)
